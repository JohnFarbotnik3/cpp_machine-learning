

* create optimized variants of the simplified network modules in the "networks"
folder by copying file, appending "_o1", "_o2", etc. and then optimizing whatever looks like
it will produce the biggest speedup.
^ in particular, reducing size of neuron structures by extracting values/value-histories from neurons,
as well as per-iteration loss, in order to get better data-locality.


* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...


> optimize autoencoder.
- add multithreading.
	^ spawn N threads.
	^ while end of neurons array not reached, each thread gets next interval to update
	(advancing shared interval-start position).
- add backtargets which point to neurons in next layer that read this neuron,
as well as to links (to get access to weights).
	^ this will allow safely multithreading backprop as well.
...

> debug autoencoder.
- implement adaptive learning rate (try increasing, decreasing, or keeping it the same, then
go with whichever produces the lowest error);
	^ this should operate on clones of the base model rather than the model itself.
	^ this can be implemented by running K training cycles with each setting to see which is the best.
	^ repeat learning-rate update function every N >> K training cylces.
- implement model saving and loading (plus a snapshot interval for intermediate model states).
- check if lowering ReLU leakage speeds of the slow half of the learning process.
	^ with lower leakage, try higher learning rate?
- add config file parsing (newline delimited KEY=VALUE pairs).
- re-add middle layers to fully prove autoencoder effectiveness.


- add "training_settings" struct to make training functions simpler / more concise.



