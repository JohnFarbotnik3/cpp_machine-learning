

* create optimized variants of the simplified network modules in the "networks"
folder by copying file, appending "_o1", "_o2", etc. and then optimizing whatever looks like
it will produce the biggest speedup.
^ in particular, reducing size of neuron structures by extracting values/value-histories from neurons,
as well as per-iteration loss, in order to get better data-locality.


* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...



> debug autoencoder.
- implement model saving and loading (plus a snapshot interval for intermediate model states).
- check if lowering ReLU leakage speeds of the slow half of the learning process.
	^ with lower leakage, try higher learning rate?
- add config file parsing (newline delimited KEY=VALUE pairs).
- re-add middle layers to fully prove autoencoder effectiveness.


> optimize autoencoder.
- add multithreading.
	^ spawn N threads.
	^ while end of neurons array not reached, each thread gets next interval to update
	(advancing shared interval-start position).
* figure out why backprop (specifically the weights part) isnt scaling well with number of threads.
	n_threads=1: foreward=60ms, backprop=70ms
	n_threads=2: foreward=31ms, backprop=57ms
* running "linux perf" and doing a couple other experiments didnt reveal anything in particular,
but I highly suspect that memory bandwidth may be a problem. I may be time to implement
image tiling iterators meanwhile also implementing images + layer-generation with variable number of channels.
(this will be an overhaul)
...









