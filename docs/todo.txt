

* create optimized variants of the simplified network modules in the "networks"
folder by copying file, appending "_o1", "_o2", etc. and then optimizing whatever looks like
it will produce the biggest speedup.
^ in particular, reducing size of neuron structures by extracting values/value-histories from neurons,
as well as per-iteration loss, in order to get better data-locality.


* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...



* create a logistics structure to pass into propagate() and backpropagate()
to collection (and visualize) information about the training process.

> training note - image output error:
- when generating output_error, area outside the image will be ignored (error=0),
as in theory we would be storing both the image embedding-vector and original image metadata.




> todo - run autoencoder.
! save images and logs!!!
x problem: mixing layers seem to work by after a single encode-decode layer-pair,
	all the images seem to blend into the same image.
	(something may be wrong with topology?)
x sometimes training goes from looking good to instant NaNs.
	figure out source of NaNs!
x figure out why model converges slowly than suddenly begins to (and continues to) diverge.
^ generate (and print) a per-layer histogram of neuron biases and weights.
^ generate images during training (currently they are only generated after training is done).
^ also generate normalized images of midlayer-activations.





