
* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> video compression
- rather than train a full video autoencoder, it would be easier to train a upscaling convolution-head
which takes in the frames [N-x, N] of the downscaled video plus [N-x, N-1] frames of the full scale video,
to predict the next frame. this can be trained to match the full video as closely as possible,
and remaining error is encoded as a loss-stream.
^ to reconstitute the video, we would need: the downscaled video, the loss stream, and the convolution-head.
- it may make sense to train an upscaling convolution-head to work on images as well.
* also experiment with LZMA for video (of particular interest are vtuber karaoke streams).


> convolution-based autoencoder.
- it may be possible to significantly beat the information density of a full image autoencoder
by training a convolution-based autoencoder instead.
- the convolution-head would see a scaled-down sample of the entire input image (ex: 64x64)
as well as an NxN patch of pixels directly beneath it (ex: 32x32),
outputting a smaller BxB patch (ex: 8x8) with additional colour channels,
scaling the input image down by some factor.
- this would be easy enough to multithread foreward-propagation as well as backward-propagation,
however it would technically require vastly more computation than the full-image AE.
* this is likely a GPU-oriented task, as GPUs handle huge numbers of computations
on shared data fairly well.
- the part of a convolution-head that computes on the image-sample
can cache the result of that computation (since the input image-sample doesnt change,
only the NxN convolution-area).
- convolution-heads may benefit from having multiple layers,
in exchange for having less encoding steps/layers to condense the image.
- convolution-heads will still have to be quite large in order to
develop an actual understanding of the images being compressed.
* backprop would involve accumulating error across all the output patches,
which I'm not sure how to effectively multithread on GPUs (likely group summing of some kind).
^ input-image can be used to re-compute input/signal history throughout layers of convolution head,
so the only history that is actually needed is aforementioned input-image.
^ perhaps spawn a thread for each row of the image;
each thread accumulates batch-error across the row,
then batch-errors accumulated by all the threads are summed together.
this would (hopefully) keep the memory requirements reasonable.
^ if convolution-heads are roughly 50MiB (large enough to begin to understand),
then memory will likely still be an issue.
^ it may be far simpler to resort to some heuristic form of back-propagated annealing.
if backprop is slower or more memory intensive by a wide enough margin, it could
actually converge faster.
^^ back-propagated annealing could work with just the overall error of the output-layer of the convolution head,
but I havent figured out how to produce an input-error image with it yet.



> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...



> alternative to graph:
- use multiple images instead of huge amorphous graph.
- each layer Z has connections pointing to layers { INPUT, OUTPUT, Z-1, Z, Z+1 }.


> graph multithreading - partitions:
- if the graph can be partitioned such that the vast majority of the connections
in each partition point to other neurons in the same partition, with only a few pointing externally,
the that would allow decent multithreading.


> optimize autoencoder.
* figure out why backprop (specifically the weights part) isnt scaling well with number of threads.
	n_threads=1: foreward=60ms, backprop=70ms
	n_threads=2: foreward=31ms, backprop=57ms
* running "perf" and doing a couple other experiments didnt reveal anything in particular (ex. "false sharing"),
but I highly suspect that memory bandwidth may be a problem. I may be time to implement
image tiling iterators meanwhile also implementing images + layer-generation with variable number of channels.
(this will be an overhaul)
* NOTE: running "perf record -- COMMAND [OPTIONS...]" then "perf report" can show
useful information about program hotspots.
- test image iterators (by printing indices and coordinates).
- create variable_image.
- use variable_image in autoencoder (mostly replacing sample_image).
*** currently connections make up majority of memory usage and memory pressure,
	however using a value+signal history equal to batch size would allow loading each
	neuron's connection list once, then processing multiple images,
	then backpropagating for each of the images.
	^ this would allow removing the batch-error accumulators, significantly reducing memory usage;
	and backprop would directly apply adjustments instead.
	^ back-targets could be removed completely and write-style
	backprop could be used (but then it would become single threaded).
	^ neurons could store history of values and signals inside itself
	so that images dont need to be interleaved.
	^ outside source can keep track of current history number,
	sending it to prop and backprop functions so they know
	which history entry to populate, or how many to backprop for.
	(this is also useful for cases where minibatches occasionally have odd sizes.)
	^ history could be stored in its own vector image, but with H times the number of channels.
	the index to access would then be neuron_index * H.
	^ fixed architecture layers would reduce memory consumption considerably as well,
	but would involve using iterators extensively, increasing cpu load.
	(each layer would have values keeping track of which iterator and iterator-parameters to use.)
	^ multi-threaded backprop may still benefit from target indices since inverse mapping
	could be complicated, particularly around edges of image.
- use memcpy instead of using "=" to set vector data,
as it may improve performance by reducing allocations.
^ WARNING: memcpy is risky since its really easy to make typos (such as forgetting to multiply by sizeof).
	write a vec_memcpy function which for copying ranges in vectors, which calls memcpy and DOESNT have typos.
- some parts of backprop are not yet multithreaded (ex: error normalization).
- there is room to introduce SIMD.


> Gaussian splatting based image compression.
* Gaussian splatting might be able to recreate images with FAR fewer parameters than an autoencoder.
^ it may make sense to break images into sections whose output contents dont spill over (output clipping).
^ drawing Gaussians may be expensive, try to find way to make it inexpensive.
^ each output tile should be able to see local part of input at full resultion,
	and whole image at some reduced resolution (for added context).
...



> misc
- add a dense connection push-layer function (all inputs connected to all outputs).
- autoencoder: experiment with lower bias values and lower ReLU leakage.
- add stochastic-back-anneal function which simply anneals with temperature
proportional to error of that particular neuron, then backprops that error.
^ this may be useful if the error function doesnt have direction information.
- graph backprop can have a queue of neurons to update during
next backprop iteration, with their respective local losses.
- fix console stuttering by buffering a few lines and then committing them at times
that would result in good looking / well aligned prints.
* idea: graph model can have both fixed architecture links and flexible links.
both would be processed during foreward & backward propagation; however this would
make implementation more complex (but faster and more memory efficient).
^^ note: switching back to scanlines would make coordinates and indices
far easier to work with.
- pick better commandline parameter names.
** storing value-history and signal-history might not be worth it,
as it limits the maximum batch size to 5-10. a good idea
for improving backprop performance might be to keep adjustments
seperate from the other values rather than interleaving them like I did before
with the layer_neuron struct.
- write more convenient thread spawning functions (variadic) so that I dont have to repeat the
same threads-over-intervals code over and over again.



> create X_O1 variant of autoencoder with fixed layer type.
	* autoencoder doesnt need flexible targeting scheme.
	* instead of having "autoencoder.cpp" in "./projects", have "./projects/autoencoder_o1" and "./.../main.cpp"
	* put current autoencoder project in "./projects/autoencoder",
		including the relevant image typedefs and loading functions;
		but dont actually move the variable_image and variable_image_tiled structs
		out of "image.cpp", as they may still see general use.
	- revert images back to being scanline-based (create non-tiled variant of image).
	- iterate over x,y coordinates.
	- store weights and weight-errors in layer itself.
	- store layer type, dimensions, etc. for proper iteration when reading from input
	during propagation and when reading to input during backprop.
	- propagate and backprop function variants will be needed for each layer type.
	- implement custom "foreward_target_list" type which stores offsets, weights, and weight-errors.
	^ when applying batch error, it is sufficient to just iterate through whole target list.
...


> multithreaded fixed-autoencoder.
* a semi-general solution to multithreading an autoencoder would be to split each image and each layer into subimages.
^ these subimages would benefit from having padding pixels (for both input and output)
which contain data from adjacent subimages, or are initialized to 0 if out of bounds
^ padding would allow getting rid of some bounds checks, as well as pre-computing tables
of offsets for input area (if thats even needed - it probably wont be with the algorithm simplifications).
^ this would get some of the benefits of tiled images too, since the scanlines would be shorter.
^ write-style operations would be less of a concern.
^ create a padded_value_image type for this. having a good get_offset(x,y,c) function would help a lot.
- there should be some minimum subimage size (ex. 64*64 or 128*128),
because otherwise thread-spawning and padding-exchange would add too much overhead.
* it may help to use the fw/bp target paradigm for better data locality during backprop,
but the target_index doesnt need to be computed; batch_apply_error can handle the inverse mapping,
and some things will be made simpler by having a *fixed* number of targets for each neuron (some of which will be unused).
^ neuron_index may not be needed either?
* we may want to store extra info in the layers for keeping track of properties of subimages.



> debug autoencoder.
- add config file parsing (newline delimited KEY=VALUE pairs).
* work on model ergonomics first:
	- config files: commandline parameters in text file.
	- model save/load: launch-config, current-settings, model-parameters, training-logs.
	- interactive console: log training stuff, but also allow typing commands,
		such as changing settings, play/pause, and save/load.
	- custom loggers: each one with a different file it accumulates into.
		every message should have format: { timestamp, log_level, text }.
		(will involve lots of snprintf, or writing variadic functions of my own.
		if writing my own, try javascript style names like "console.log(...)".)
- add commandline arguments for model-parameter distributions.
* experiment with different colour-channel formats, example RGB, HSL, etc...
...


> subimage autoencoder
** read SIMD optimization guide, as there may be many very suboptimal things I am doing that I dont even know about.
* pass training stats to model functions to allow for more granularity of timing measurements.
* implement vec16f and larger batches to observe if performance uplift continues (since right now weights still seem to be majority of memory bandwidth).
- add getter for WEIGHTS_PER_NEURON to layer_pattern struct.


> add to journal:
- switching from ReLU with leakage to piecewise sigmoid to fix bias degeneration
	(biases tending towards larger values as backprop works it way to input layer).
- optimization: skipping backprop on pixels with 0 output-side error.
	^ this speeds up backprop enough that simpler write-oriented algorithm is probably sufficient.
- optimization: interleaving mini-batches of images at the pixel-value level for optimal memory access and SIMD.
	^ this renders many of my previous methods of optimization obsolete.
...




