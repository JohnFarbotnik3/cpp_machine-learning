

* create optimized variants of the simplified network modules in the "networks"
folder by copying file, appending "_o1", "_o2", etc. and then optimizing whatever looks like
it will produce the biggest speedup.
^ in particular, reducing size of neuron structures by extracting values/value-histories from neurons,
as well as per-iteration loss, in order to get better data-locality.


* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...



> alternative to graph:
- use multiple images instead of huge amorphous graph.
- each layer Z has connections pointing to layers { INPUT, OUTPUT, Z-1, Z, Z+1 }.


> graph multithreading - partitions:
- if the graph can be partitioned such that the vast majority of the connections
in each partition point to other neurons in the same partition, with only a few pointing externally,
the that would allow decent multithreading.


> optimize autoencoder.
* figure out why backprop (specifically the weights part) isnt scaling well with number of threads.
	n_threads=1: foreward=60ms, backprop=70ms
	n_threads=2: foreward=31ms, backprop=57ms
* running "perf" and doing a couple other experiments didnt reveal anything in particular (ex. "false sharing"),
but I highly suspect that memory bandwidth may be a problem. I may be time to implement
image tiling iterators meanwhile also implementing images + layer-generation with variable number of channels.
(this will be an overhaul)
* NOTE: running "perf record -- COMMAND [OPTIONS...]" then "perf report" can show
useful information about program hotspots.
- test image iterators (by printing indices and coordinates).
- create variable_image.
- use variable_image in autoencoder (mostly replacing sample_image).
*** currently connections make up majority of memory usage and memory pressure,
	however using a value+signal history equal to batch size would allow loading each
	neuron's connection list once, then processing multiple images,
	then backpropagating for each of the images.
	^ this would allow removing the batch-error accumulators, significantly reducing memory usage;
	and backprop would directly apply adjustments instead.
	^ back-targets could be removed completely and write-style
	backprop could be used (but then it would become single threaded).
	^ neurons could store history of values and signals inside itself
	so that images dont need to be interleaved.
	^ outside source can keep track of current history number,
	sending it to prop and backprop functions so they know
	which history entry to populate, or how many to backprop for.
	(this is also useful for cases where minibatches occasionally have odd sizes.)
	^ history could be stored in its own vector image, but with H times the number of channels.
	the index to access would then be neuron_index * H.
	^ fixed architecture layers would reduce memory consumption considerably as well,
	but would involve using iterators extensively, increasing cpu load.
	(each layer would have values keeping track of which iterator and iterator-parameters to use.)
	^ multi-threaded backprop may still benefit from target indices since inverse mapping
	could be complicated, particularly around edges of image.
- use memcpy instead of using "=" to set vector data,
as it may improve performance by reducing allocations.
^ WARNING: memcpy is risky since its really easy to make typos (such as forgetting to multiply by sizeof).
	write a vec_memcpy function which for copying ranges in vectors, which calls memcpy and DOESNT have typos.
- some parts of backprop are not yet multithreaded (ex: error normalization).
- there is room to introduce SIMD.



> misc
- add a dense connection push-layer function (all inputs connected to all outputs).
- autoencoder: experiment with lower bias values and lower ReLU leakage.
- add stochastic-back-anneal function which simply anneals with temperature
proportional to error of that particular neuron, then backprops that error.
^ this may be useful if the error function doesnt have direction information.
- graph backprop can have a queue of neurons to update during
next backprop iteration, with their respective local losses.
- fix console stuttering by buffering a few lines and then committing them at times
that would result in good looking / well aligned prints.
* idea: graph model can have both fixed architecture links and flexible links.
both would be processed during foreward & backward propagation; however this would
make implementation more complex (but faster and more memory efficient).
^^ note: switching back to scanlines would make coordinates and indices
far easier to work with.
- pick better commandline parameter names.
** storing value-history and signal-history might not be worth it,
as it limits the maximum batch size to 5-10. a good idea
for improving backprop performance might be to keep adjustments
seperate from the other values rather than interleaving them like I did before
with the layer_neuron struct.
- write more convenient thread spawning functions (variadic) so that I dont have to repeat the
same threads-over-intervals code over and over again.



> create X_O1 variant of autoencoder with fixed layer type.
	* autoencoder doesnt need flexible targeting scheme.
	* instead of having "autoencoder.cpp" in "./projects", have "./projects/autoencoder_o1" and "./.../main.cpp"
	* put current autoencoder project in "./projects/autoencoder",
		including the relevant image typedefs and loading functions;
		but dont actually move the variable_image and variable_image_tiled structs
		out of "image.cpp", as they may still see general use.
	- revert images back to being scanline-based (create non-tiled variant of image).
	- iterate over x,y coordinates.
	- store weights and weight-errors in layer itself.
	- store layer type, dimensions, etc. for proper iteration when reading from input
	during propagation and when reading to input during backprop.
	- propagate and backprop function variants will be needed for each layer type.
	- implement custom "foreward_target_list" type which stores offsets, weights, and weight-errors.
	^ when applying batch error, it is sufficient to just iterate through whole target list.
...


> debug autoencoder.
- check if lowering ReLU leakage speeds up the slow half of the learning process.
	^ with lower leakage, try higher learning rate?
- add config file parsing (newline delimited KEY=VALUE pairs).
- re-add middle layers to fully prove autoencoder effectiveness.
- figure out why model is performing well and then explodes without warning
far into the training process (when error values are very small).
* work on model ergonomics first:
	- config files: commandline parameters in text file.
	- model save/load: launch-config, current-settings, model-parameters, training-logs.
	- interactive console: log training stuff, but also allow typing commands,
		such as changing settings, play/pause, and save/load.
	- custom loggers: each one with a different file it accumulates into.
		every message should have format: { timestamp, log_level, text }.
		(will involve lots of snprintf, or writing variadic functions of my own.
		if writing my own, try javascript style names like "console.log(...)".)
* fix seemingly dead/unresponsive pixels at end of long training runs.
	for some reason the last few pixels just dont seem to train.
^ (adaptive) annealing may help with this, i.e. annealing the model periodically
	with an annealing-rate that adjusts the same way learning-rate does.
	backpropagated-annealing may fix this much more quickly than naive annealing,
	but that would involve duplicating the backprop code which would be annoying.
	(unless I clean up and simplify the code enough.)
** experiment with reducing loss power (continuously), starting from L^2.0 at high error
and tending towards L^1.0 as error reaches 0. when combined with capped adaptive learning
rate this should get model to actually finish training the "dead pixels" (I hope).
** add simple annealing, and an interval over which to compare training
	of model variants with different annealing seeds & strengths (ex. LR*0, LR*0.5, LR*1.0, LR*2.0).
** try smaller bias distribution (reduced stdev),
	and add commandline arguments for model-parameter distributions.
** clamp output image values when generating image files / previews.
x long run with 30 images crashed at z=~1000/2000, see logs.
...



> write journal post:
- why forewardprop-targets (as opposed to fixed architecture).
- why backprop-targets (to allow multithreading - "read style" vs "write style").
- adaptive learning rate implementation.
- why ReLU with leakage.
- backprop error gradient normalization (a potential issue with concentrating error gradient).
- why tiled images (show pictures with scanline vs tiled memory layouts).
- current issues
	- model degeneration - suspected to be caused by an interaction between loss_squared and error gradient concentration.
	- "dead pixels" - it seems that they were caused by underflow and overflow, which was fixed by clamping output preview images.
	- switching to fixed layers.
...







