

* create optimized variants of the simplified network modules in the "networks"
folder by copying file, appending "_o1", "_o2", etc. and then optimizing whatever looks like
it will produce the biggest speedup.
^ in particular, reducing size of neuron structures by extracting values/value-histories from neurons,
as well as per-iteration loss, in order to get better data-locality.


* how to train weights and biases:
https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation


> create visualizations showing the actual neural network itself.
- have rendering functions drawing layers as a W x H grid of neurons.
- also draw connections and activation values.
* this can help locate and troubleshoot intialization bugs/typos.
* this would also be very pretty.
- layers and graphs will require slightly (or significantly) different drawing logic.
* allow directly modifying model parameters by clicking on a neuron or connection.
^ when neuron or connection is clicked, it is added to a list.
^ each list item will have a group of sliders/text-inputs for modifying value.


* it might make sense to add W x H dimensions to the sub-networks (layer/graph) themselves,
as it gives clues about what the layer is for, as well as hinting at preferred memory layout.



> frontend:
! start by implementing and testing as a CLI program, then try to connect it to a browser after.
* NOTE: model input data will be put in a folder near the application binary
^ TODO: add config file with:
	- input-directory
	- model-directory
	- output-directory (for when user saves autoencoder image outputs)
- controls:
	- slider: learning rate (exponential scaling)
	- slider: batch size
	- buttons: reset / start / stop
- logistics:
	- graph: error
	- div: time taken per iteration / mini-batch / full-batch
	- div: # files in dataset / total size (MiB)
	- div: # neurons / # connections
	- image: input / output
	- canvas: model render (neurons and connections)
- model interactions (at a later-stage in project):
	- manipulate embedding values then propage decoder
	- set custom input image then run autoencoder (without applying training)
...



> alternative to graph:
- use multiple images instead of huge amorphous graph.
- each layer Z has connections pointing to layers { INPUT, OUTPUT, Z-1, Z, Z+1 }.


> graph multithreading - partitions:
- if the graph can be partitioned such that the vast majority of the connections
in each partition point to other neurons in the same partition, with only a few pointing externally,
the that would allow decent multithreading.


> debug autoencoder.
- implement model saving and loading (plus a snapshot interval for intermediate model states).
- check if lowering ReLU leakage speeds of the slow half of the learning process.
	^ with lower leakage, try higher learning rate?
- add config file parsing (newline delimited KEY=VALUE pairs).
- re-add middle layers to fully prove autoencoder effectiveness.
- bug: trying "-channels 3" causes buggy output.

> optimize autoencoder.
* figure out why backprop (specifically the weights part) isnt scaling well with number of threads.
	n_threads=1: foreward=60ms, backprop=70ms
	n_threads=2: foreward=31ms, backprop=57ms
* running "perf" and doing a couple other experiments didnt reveal anything in particular (ex. "false sharing"),
but I highly suspect that memory bandwidth may be a problem. I may be time to implement
image tiling iterators meanwhile also implementing images + layer-generation with variable number of channels.
(this will be an overhaul)
* NOTE: running "perf record -- COMMAND [OPTIONS...]" then "perf report" can show
useful information about program hotspots.
- test image iterators (by printing indices and coordinates).
- create variable_image.
- use variable_image in autoencoder (mostly replacing sample_image).
*** currently connections make up majority of memory usage and memory pressure,
	however using a value+signal history equal to batch size would allow loading each
	neuron's connection list once, then processing multiple images,
	then backpropagating for each of the images.
	^ this would allow removing the batch-error accumulators, significantly reducing memory usage;
	and backprop would directly apply adjustments instead.
	^ back-targets could be removed completely and write-style
	backprop could be used (but then it would become single threaded).
	^ neurons could store history of values and signals inside itself
	so that images dont need to be interleaved.
	^ outside source can keep track of current history number,
	sending it to prop and backprop functions so they know
	which history entry to populate, or how many to backprop for.
	(this is also useful for cases where minibatches occasionally have odd sizes.)
	^ history could be stored in its own vector image, but with H times the number of channels.
	the index to access would then be neuron_index * H.
	^ fixed architecture layers would reduce memory consumption considerably as well,
	but would involve using iterators extensively, increasing cpu load.
	(each layer would have values keeping track of which iterator and iterator-parameters to use.)
	^ multi-threaded backprop may still benefit from target indices since inverse mapping
	could be complicated, particularly around edges of image.
...


> misc
- add a dense connection push-layer function (all inputs connected to all outputs).
- autoencoder: experiment with lower bias values and lower ReLU leakage.
- use different tiling in push-layer value-images based on whether or not values
are being mixed.







